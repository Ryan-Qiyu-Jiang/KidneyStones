{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abcd_experiments.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgM3Z9e5Rl+f4hOCiL1bnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryan-Qiyu-Jiang/KidneyStones/blob/master/abcd_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJkMKaEYjPhd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Ryan-Qiyu-Jiang/rloss.git\n",
        "%cd /content/rloss/\n",
        "!git checkout color_query"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/rloss/data/VOC2012/\n",
        "!./fetchVOC2012.sh\n",
        "%cd /content/rloss/data/pascal_scribble/\n",
        "! ./fetchPascalScribble.sh"
      ],
      "metadata": {
        "id": "x4-ND59tjZ4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq tensorboardX\n",
        "!pip install -qqq wandb\n",
        "!pip install -qqq pytorch-lightning\n",
        "!pip install -qqq kornia"
      ],
      "metadata": {
        "id": "ovWMQ0j9jgy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/rloss/pytorch/pytorch_deeplab_v3_plus"
      ],
      "metadata": {
        "id": "J8WVxn2-1KZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from mypath import Path\n",
        "from dataloaders import make_data_loader\n",
        "from dataloaders.custom_transforms import denormalizeimage\n",
        "from modeling.sync_batchnorm.replicate import patch_replication_callback\n",
        "from modeling.deeplab import *\n",
        "from utils.loss import SegmentationLosses\n",
        "from utils.calculate_weights import calculate_weigths_labels\n",
        "from utils.lr_scheduler import LR_Scheduler\n",
        "from utils.saver import Saver\n",
        "from utils.summaries import TensorboardSummary\n",
        "from utils.metrics import Evaluator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torch.nn import functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch\n",
        "from argparse import Namespace\n",
        "from dataloaders.utils import decode_seg_map_sequence\n",
        "\n",
        "from color_query import BaseModel, get_args, SingleDataset, RepeatDataset, DeepLabEncoder, BaseColorModel\n",
        "\n",
        "segmentation_classes = [\n",
        "    'background','aeroplane','bicycle','bird','boat','bottle',\n",
        "    'bus','car','cat','chair','cow','diningtable','dog','horse',\n",
        "    'motorbike','person','pottedplant','sheep','sofa','train','tvmonitor'\n",
        "]\n",
        "\n",
        "def labels():\n",
        "  l = {}\n",
        "  for i, label in enumerate(segmentation_classes):\n",
        "    l[i] = label\n",
        "  return l\n",
        "\n",
        "def wb_mask(bg_img, pred_mask, true_mask):\n",
        "  return wandb.Image(bg_img, masks={\n",
        "    \"prediction\" : {\"mask_data\" : pred_mask, \"class_labels\" : labels()},\n",
        "    \"ground truth\" : {\"mask_data\" : true_mask, \"class_labels\" : labels()}})"
      ],
      "metadata": {
        "id": "O5uQZJxtrSoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "from dataloaders import make_data_loader\n",
        "\n",
        "args_dict = get_args()\n",
        "args_dict['cuda'] = True\n",
        "args_dict['checkname'] = 'ignore'\n",
        "args_dict['epochs'] = 1\n",
        "args_dict['shuffle'] = False # True for real training\n",
        "args_dict['batch_size'] = 10\n",
        "args_dict['lr'] = 1e-3\n",
        "args_dict['full_gt'] = True # True for gt\n",
        "args_dict['limit_dataset'] = False\n",
        "# args_dict['rloss_scale'] = 1\n",
        "args = Namespace(**args_dict)\n",
        "\n",
        "kwargs = {'num_workers': 6, 'pin_memory': True}\n",
        "train_loader, val_loader, test_loader, nclass = make_data_loader(args, **kwargs)\n",
        "gt_loader = val_loader\n",
        "\n",
        "seeds_args = Namespace(**dict(args_dict, full_gt=False) )\n",
        "seeds_loader, _, _, _ = make_data_loader(seeds_args, **kwargs)"
      ],
      "metadata": {
        "id": "XdbQ5byZrK2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "batch = iter(seeds_loader).next()\n",
        "batch_sample = {k:v for k,v in batch.items()}\n",
        "bs = 10\n",
        "single_dataset = RepeatDataset(batch_sample,  100*bs)\n",
        "single_train_loader = DataLoader(single_dataset, batch_size=bs, shuffle=True, num_workers=4)\n",
        "single_val_loader = DataLoader(single_dataset, batch_size=bs, shuffle=False, num_workers=4)\n",
        "\n",
        "plt.imshow(batch['image'][0].numpy().transpose(1,2,0));"
      ],
      "metadata": {
        "id": "KIeBG7-htBlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "eOxoX-Xb03LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "from utils.metrics import Evaluator\n",
        "\n",
        "\n",
        "def to_img(img_tensor, **kwargs):\n",
        "  if img_tensor.size(0) == 3:\n",
        "    img_tensor = img_tensor.permute(1, 2, 0) # 3 h w -> h w 3\n",
        "  return wandb.Image((img_tensor.cpu().detach().numpy() * 255).astype(np.uint8), **kwargs)\n",
        "log_num = 0\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FFCSE_block(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, ratio_g):\n",
        "        super(FFCSE_block, self).__init__()\n",
        "        in_cg = int(channels * ratio_g)\n",
        "        in_cl = channels - in_cg\n",
        "        r = 16\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.conv1 = nn.Conv2d(channels, channels // r,\n",
        "                               kernel_size=1, bias=True)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv_a2l = None if in_cl == 0 else nn.Conv2d(\n",
        "            channels // r, in_cl, kernel_size=1, bias=True)\n",
        "        self.conv_a2g = None if in_cg == 0 else nn.Conv2d(\n",
        "            channels // r, in_cg, kernel_size=1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x if type(x) is tuple else (x, 0)\n",
        "        id_l, id_g = x\n",
        "\n",
        "        x = id_l if type(id_g) is int else torch.cat([id_l, id_g], dim=1)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.relu1(self.conv1(x))\n",
        "\n",
        "        x_l = 0 if self.conv_a2l is None else id_l * \\\n",
        "            self.sigmoid(self.conv_a2l(x))\n",
        "        x_g = 0 if self.conv_a2g is None else id_g * \\\n",
        "            self.sigmoid(self.conv_a2g(x))\n",
        "        return x_l, x_g\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        res = x * y.expand_as(x)\n",
        "        return res\n",
        "\n",
        "class FourierUnit(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, groups=1, spatial_scale_factor=None, spatial_scale_mode='bilinear',\n",
        "                 spectral_pos_encoding=False, use_se=False, se_kwargs=None, ffc3d=False, fft_norm='ortho'):\n",
        "        # bn_layer not used\n",
        "        super(FourierUnit, self).__init__()\n",
        "        self.groups = groups\n",
        "\n",
        "        self.conv_layer = torch.nn.Conv2d(in_channels=in_channels * 2 + (2 if spectral_pos_encoding else 0),\n",
        "                                          out_channels=out_channels * 2,\n",
        "                                          kernel_size=1, stride=1, padding=0, groups=self.groups, bias=False)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels * 2)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "        # squeeze and excitation block\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            if se_kwargs is None:\n",
        "                se_kwargs = {}\n",
        "            self.se = SELayer(self.conv_layer.in_channels, **se_kwargs)\n",
        "\n",
        "        self.spatial_scale_factor = spatial_scale_factor\n",
        "        self.spatial_scale_mode = spatial_scale_mode\n",
        "        self.spectral_pos_encoding = spectral_pos_encoding\n",
        "        self.ffc3d = ffc3d\n",
        "        self.fft_norm = fft_norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch = x.shape[0]\n",
        "\n",
        "        if self.spatial_scale_factor is not None:\n",
        "            orig_size = x.shape[-2:]\n",
        "            x = F.interpolate(x, scale_factor=self.spatial_scale_factor, mode=self.spatial_scale_mode, align_corners=False)\n",
        "\n",
        "        r_size = x.size()\n",
        "        # (batch, c, h, w/2+1, 2)\n",
        "        fft_dim = (-3, -2, -1) if self.ffc3d else (-2, -1)\n",
        "        ffted = torch.fft.rfftn(x, dim=fft_dim, norm=self.fft_norm)\n",
        "        ffted = torch.stack((ffted.real, ffted.imag), dim=-1)\n",
        "        ffted = ffted.permute(0, 1, 4, 2, 3).contiguous()  # (batch, c, 2, h, w/2+1)\n",
        "        ffted = ffted.view((batch, -1,) + ffted.size()[3:])\n",
        "\n",
        "        if self.spectral_pos_encoding:\n",
        "            height, width = ffted.shape[-2:]\n",
        "            coords_vert = torch.linspace(0, 1, height)[None, None, :, None].expand(batch, 1, height, width).to(ffted)\n",
        "            coords_hor = torch.linspace(0, 1, width)[None, None, None, :].expand(batch, 1, height, width).to(ffted)\n",
        "            ffted = torch.cat((coords_vert, coords_hor, ffted), dim=1)\n",
        "\n",
        "        if self.use_se:\n",
        "            ffted = self.se(ffted)\n",
        "\n",
        "        ffted = self.conv_layer(ffted)  # (batch, c*2, h, w/2+1)\n",
        "        ffted = self.relu(self.bn(ffted))\n",
        "\n",
        "        ffted = ffted.view((batch, -1, 2,) + ffted.size()[2:]).permute(\n",
        "            0, 1, 3, 4, 2).contiguous()  # (batch,c, t, h, w/2+1, 2)\n",
        "        ffted = torch.complex(ffted[..., 0], ffted[..., 1])\n",
        "\n",
        "        ifft_shape_slice = x.shape[-3:] if self.ffc3d else x.shape[-2:]\n",
        "        output = torch.fft.irfftn(ffted, s=ifft_shape_slice, dim=fft_dim, norm=self.fft_norm)\n",
        "\n",
        "        if self.spatial_scale_factor is not None:\n",
        "            output = F.interpolate(output, size=orig_size, mode=self.spatial_scale_mode, align_corners=False)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SpectralTransform(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, groups=1, enable_lfu=True, **fu_kwargs):\n",
        "        # bn_layer not used\n",
        "        super(SpectralTransform, self).__init__()\n",
        "        self.enable_lfu = enable_lfu\n",
        "        if stride == 2:\n",
        "            self.downsample = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n",
        "        else:\n",
        "            self.downsample = nn.Identity()\n",
        "\n",
        "        self.stride = stride\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels //\n",
        "                      2, kernel_size=1, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_channels // 2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.fu = FourierUnit(\n",
        "            out_channels // 2, out_channels // 2, groups, **fu_kwargs)\n",
        "        if self.enable_lfu:\n",
        "            self.lfu = FourierUnit(\n",
        "                out_channels // 2, out_channels // 2, groups)\n",
        "        self.conv2 = torch.nn.Conv2d(\n",
        "            out_channels // 2, out_channels, kernel_size=1, groups=groups, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # import pdb;pdb.set_trace()\n",
        "        x = self.downsample(x)\n",
        "        x = self.conv1(x)\n",
        "        output = self.fu(x)\n",
        "\n",
        "        if self.enable_lfu:\n",
        "            n, c, h, w = x.shape\n",
        "            split_no = 2\n",
        "            split_s = h // split_no\n",
        "            xs = torch.cat(torch.split(\n",
        "                x[:, :c // 4], split_s, dim=-2), dim=1).contiguous()\n",
        "            xs = torch.cat(torch.split(xs, split_s, dim=-1),\n",
        "                           dim=1).contiguous()\n",
        "            xs = self.lfu(xs)\n",
        "            xs = xs.repeat(1, 1, split_no, split_no).contiguous()\n",
        "        else:\n",
        "            xs = 0\n",
        "\n",
        "        output = self.conv2(x + output + xs)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class FFC(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 ratio_gin, ratio_gout, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=False, enable_lfu=True,\n",
        "                 padding_type='reflect', gated=False, **spectral_kwargs):\n",
        "        super(FFC, self).__init__()\n",
        "\n",
        "        assert stride == 1 or stride == 2, \"Stride should be 1 or 2.\"\n",
        "        self.stride = stride\n",
        "\n",
        "        in_cg = int(in_channels * ratio_gin)\n",
        "        in_cl = in_channels - in_cg\n",
        "        out_cg = int(out_channels * ratio_gout)\n",
        "        out_cl = out_channels - out_cg\n",
        "        #groups_g = 1 if groups == 1 else int(groups * ratio_gout)\n",
        "        #groups_l = 1 if groups == 1 else groups - groups_g\n",
        "\n",
        "        self.ratio_gin = ratio_gin\n",
        "        self.ratio_gout = ratio_gout\n",
        "        self.global_in_num = in_cg\n",
        "\n",
        "        module = nn.Identity if in_cl == 0 or out_cl == 0 else nn.Conv2d\n",
        "        self.convl2l = module(in_cl, out_cl, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias, padding_mode=padding_type)\n",
        "        module = nn.Identity if in_cl == 0 or out_cg == 0 else nn.Conv2d\n",
        "        self.convl2g = module(in_cl, out_cg, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias, padding_mode=padding_type)\n",
        "        module = nn.Identity if in_cg == 0 or out_cl == 0 else nn.Conv2d\n",
        "        self.convg2l = module(in_cg, out_cl, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias, padding_mode=padding_type)\n",
        "        module = nn.Identity if in_cg == 0 or out_cg == 0 else SpectralTransform\n",
        "        self.convg2g = module(\n",
        "            in_cg, out_cg, stride, 1 if groups == 1 else groups // 2, enable_lfu, **spectral_kwargs)\n",
        "\n",
        "        self.gated = gated\n",
        "        module = nn.Identity if in_cg == 0 or out_cl == 0 or not self.gated else nn.Conv2d\n",
        "        self.gate = module(in_channels, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_l, x_g = x if type(x) is tuple else (x, 0)\n",
        "        out_xl, out_xg = 0, 0\n",
        "\n",
        "        if self.gated:\n",
        "            total_input_parts = [x_l]\n",
        "            if torch.is_tensor(x_g):\n",
        "                total_input_parts.append(x_g)\n",
        "            total_input = torch.cat(total_input_parts, dim=1)\n",
        "\n",
        "            gates = torch.sigmoid(self.gate(total_input))\n",
        "            g2l_gate, l2g_gate = gates.chunk(2, dim=1)\n",
        "        else:\n",
        "            g2l_gate, l2g_gate = 1, 1\n",
        "\n",
        "        if self.ratio_gout != 1:\n",
        "            out_xl = self.convl2l(x_l) + self.convg2l(x_g) * g2l_gate\n",
        "        if self.ratio_gout != 0:\n",
        "            out_xg = self.convl2g(x_l) * l2g_gate + self.convg2g(x_g)\n",
        "\n",
        "        return out_xl, out_xg\n",
        "\n",
        "\n",
        "class FFC_BN_ACT(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 kernel_size, ratio_gin, ratio_gout,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=False,\n",
        "                 norm_layer=nn.BatchNorm2d, activation_layer=nn.Identity,\n",
        "                 padding_type='reflect',\n",
        "                 enable_lfu=True, **kwargs):\n",
        "        super(FFC_BN_ACT, self).__init__()\n",
        "        self.ffc = FFC(in_channels, out_channels, kernel_size,\n",
        "                       ratio_gin, ratio_gout, stride, padding, dilation,\n",
        "                       groups, bias, enable_lfu, padding_type=padding_type, **kwargs)\n",
        "        lnorm = nn.Identity if ratio_gout == 1 else norm_layer\n",
        "        gnorm = nn.Identity if ratio_gout == 0 else norm_layer\n",
        "        global_channels = int(out_channels * ratio_gout)\n",
        "        self.bn_l = lnorm(out_channels - global_channels)\n",
        "        self.bn_g = gnorm(global_channels)\n",
        "\n",
        "        lact = nn.Identity if ratio_gout == 1 else activation_layer\n",
        "        gact = nn.Identity if ratio_gout == 0 else activation_layer\n",
        "        self.act_l = lact(inplace=True)\n",
        "        self.act_g = gact(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_l, x_g = self.ffc(x)\n",
        "        x_l = self.act_l(self.bn_l(x_l))\n",
        "        x_g = self.act_g(self.bn_g(x_g))\n",
        "        return x_l, x_g\n",
        "\n",
        "from kornia.geometry.transform import rotate\n",
        "class LearnableSpatialTransformWrapper(nn.Module):\n",
        "    def __init__(self, impl, pad_coef=0.5, angle_init_range=80, train_angle=True):\n",
        "        super().__init__()\n",
        "        self.impl = impl\n",
        "        self.angle = torch.rand(1) * angle_init_range\n",
        "        if train_angle:\n",
        "            self.angle = nn.Parameter(self.angle, requires_grad=True)\n",
        "        self.pad_coef = pad_coef\n",
        "\n",
        "    def forward(self, x):\n",
        "        if torch.is_tensor(x):\n",
        "            return self.inverse_transform(self.impl(self.transform(x)), x)\n",
        "        elif isinstance(x, tuple):\n",
        "            x_trans = tuple(self.transform(elem) for elem in x)\n",
        "            y_trans = self.impl(x_trans)\n",
        "            return tuple(self.inverse_transform(elem, orig_x) for elem, orig_x in zip(y_trans, x))\n",
        "        else:\n",
        "            raise ValueError(f'Unexpected input type {type(x)}')\n",
        "\n",
        "    def transform(self, x):\n",
        "        height, width = x.shape[2:]\n",
        "        pad_h, pad_w = int(height * self.pad_coef), int(width * self.pad_coef)\n",
        "        x_padded = F.pad(x, [pad_w, pad_w, pad_h, pad_h], mode='reflect')\n",
        "        x_padded_rotated = rotate(x_padded, angle=self.angle.to(x_padded))\n",
        "        return x_padded_rotated\n",
        "\n",
        "    def inverse_transform(self, y_padded_rotated, orig_x):\n",
        "        height, width = orig_x.shape[2:]\n",
        "        pad_h, pad_w = int(height * self.pad_coef), int(width * self.pad_coef)\n",
        "\n",
        "        y_padded = rotate(y_padded_rotated, angle=-self.angle.to(y_padded_rotated))\n",
        "        y_height, y_width = y_padded.shape[2:]\n",
        "        y = y_padded[:, :, pad_h : y_height - pad_h, pad_w : y_width - pad_w]\n",
        "        return y\n",
        "\n",
        "class FFCResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type, norm_layer, activation_layer=nn.ReLU, dilation=1,\n",
        "                 spatial_transform_kwargs=None, inline=False, **conv_kwargs):\n",
        "        super().__init__()\n",
        "        self.conv1 = FFC_BN_ACT(dim, dim, kernel_size=3, padding=dilation, dilation=dilation,\n",
        "                                norm_layer=norm_layer,\n",
        "                                activation_layer=activation_layer,\n",
        "                                padding_type=padding_type, \n",
        "                                **conv_kwargs)\n",
        "        self.conv2 = FFC_BN_ACT(dim, dim, kernel_size=3, padding=dilation, dilation=dilation,\n",
        "                                norm_layer=norm_layer,\n",
        "                                activation_layer=activation_layer,\n",
        "                                padding_type=padding_type,\n",
        "                                **conv_kwargs)\n",
        "        if spatial_transform_kwargs is not None:\n",
        "            self.conv1 = LearnableSpatialTransformWrapper(self.conv1, **spatial_transform_kwargs)\n",
        "            self.conv2 = LearnableSpatialTransformWrapper(self.conv2, **spatial_transform_kwargs)\n",
        "        self.inline = inline\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.inline:\n",
        "            x_l, x_g = x[:, :-self.conv1.ffc.global_in_num], x[:, -self.conv1.ffc.global_in_num:]\n",
        "        else:\n",
        "            x_l, x_g = x if type(x) is tuple else (x, 0)\n",
        "\n",
        "        id_l, id_g = x_l, x_g\n",
        "\n",
        "        x_l, x_g = self.conv1((x_l, x_g))\n",
        "        x_l, x_g = self.conv2((x_l, x_g))\n",
        "\n",
        "        x_l, x_g = id_l + x_l, id_g + x_g\n",
        "        out = x_l, x_g\n",
        "        if self.inline:\n",
        "            out = torch.cat(out, dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConcatTupleLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        assert isinstance(x, tuple)\n",
        "        x_l, x_g = x\n",
        "        assert torch.is_tensor(x_l) or torch.is_tensor(x_g)\n",
        "        if not torch.is_tensor(x_g):\n",
        "            return x_l\n",
        "        return torch.cat(x, dim=1)\n",
        "\n",
        "\n",
        "# class FFC_Decoder(nn.Module):\n",
        "#     def __init__(self, num_classes, backbone, BatchNorm):\n",
        "#         super(FFC_Decoder, self).__init__()\n",
        "#         if backbone == 'resnet' or backbone == 'drn':\n",
        "#             low_level_inplanes = 256\n",
        "#         elif backbone == 'xception':\n",
        "#             low_level_inplanes = 128\n",
        "#         elif backbone == 'mobilenet':\n",
        "#             low_level_inplanes = 24\n",
        "#         else:\n",
        "#             raise NotImplementedError\n",
        "\n",
        "#         self.ffc_low = FFC_BN_ACT(low_level_inplanes, 48, kernel_size=7, padding=0, activation_layer=nn.ReLU, ratio_gin=0, ratio_gout=0)\n",
        "#         self.last_ffc_conv = nn.Sequential(\n",
        "#             FFCResnetBlock(304, padding_type='reflect', norm_layer=BatchNorm, activation_layer=nn.ReLU, ratio_gin=0.75, ratio_gout=0.75),\n",
        "#             ConcatTupleLayer(),\n",
        "#             nn.Conv2d(304, num_classes, kernel_size=7, padding=0) #FFC_BN_ACT(low_level_inplanes, 48, kernel_size=7, padding=0, activation_layer=nn.ReLU)\n",
        "#         )\n",
        "#         self.concat_tuple = ConcatTupleLayer()\n",
        "\n",
        "#     def forward(self, x, low_level_feat):\n",
        "#         low_level_feat = self.concat_tuple(self.ffc_low(low_level_feat))\n",
        "\n",
        "#         x = F.interpolate(x, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)\n",
        "#         x = torch.cat((x, low_level_feat), dim=1)\n",
        "#         x = self.last_ffc_conv(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# from ffc import FFCResnetBlock, ConcatTupleLayer, FFC_BN_ACT # FFC_Decoder\n",
        "class FFC_Decoder(nn.Module):\n",
        "    def __init__(self, num_classes, backbone, BatchNorm):\n",
        "        super(FFC_Decoder, self).__init__()\n",
        "        if backbone == 'resnet' or backbone == 'drn':\n",
        "            low_level_inplanes = 256\n",
        "        elif backbone == 'xception':\n",
        "            low_level_inplanes = 128\n",
        "        elif backbone == 'mobilenet':\n",
        "            low_level_inplanes = 24\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.ffc_low = FFC_BN_ACT(low_level_inplanes, 48, kernel_size=1, padding=0, activation_layer=nn.ReLU, ratio_gin=0, ratio_gout=0)\n",
        "        num_channels = 256+48\n",
        "        self.last_ffc_conv = nn.Sequential(\n",
        "            FFC_BN_ACT(num_channels, num_channels, kernel_size=1, padding=0, activation_layer=nn.ReLU, ratio_gin=0, ratio_gout=0.9),\n",
        "            FFCResnetBlock(num_channels, padding_type='reflect', norm_layer=BatchNorm, activation_layer=nn.ReLU, ratio_gin=0.9, ratio_gout=0.9, enable_lfu=False),\n",
        "            ConcatTupleLayer(),\n",
        "            nn.Conv2d(num_channels, num_classes, kernel_size=1, padding=0) #FFC_BN_ACT(low_level_inplanes, 48, kernel_size=7, padding=0, activation_layer=nn.ReLU)\n",
        "        )\n",
        "        self.concat_tuple = ConcatTupleLayer()\n",
        "\n",
        "    def forward(self, x, low_level_feat):\n",
        "        # import pdb;pdb.set_trace()\n",
        "        low_level_feat = self.ffc_low(low_level_feat)\n",
        "\n",
        "        x = F.interpolate(x, size=low_level_feat[0].size()[2:], mode='bilinear', align_corners=True)\n",
        "        x_l = torch.cat((x, low_level_feat[0]), dim=1)\n",
        "        x_g = low_level_feat[1]\n",
        "        x = self.last_ffc_conv( (x_l, x_g) )\n",
        "        return x\n",
        "\n",
        "class ColorModel(BaseModel):\n",
        "    def __init__(self, hparams, encoder=None):\n",
        "        super().__init__(hparams)\n",
        "        if encoder is None:\n",
        "          model = DeepLab(num_classes=self.hparams.nclass,\n",
        "                              backbone=self.hparams.backbone,\n",
        "                              output_stride=self.hparams.out_stride,\n",
        "                              sync_bn=self.hparams.sync_bn,\n",
        "                              freeze_bn=self.hparams.freeze_bn)\n",
        "          encoder = model.backbone\n",
        "        self.encoder = encoder\n",
        "        # self.decoder = MultiColorDecoder(num_classes=self.hparams.nclass, feature_dim=256) # resnet feature map dim 320, aspp=256\n",
        "        self.decoder = FFC_Decoder(num_classes=self.hparams.nclass, backbone=self.hparams.backbone, BatchNorm=nn.BatchNorm2d)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      feature_map, low_level_feats = self.encoder(x)\n",
        "      y = self.decoder(feature_map, low_level_feats)\n",
        "      return F.interpolate(y, size=x.size()[2:], mode='bilinear', align_corners=True)\n",
        "\n",
        "    def get_loss(self, batch, batch_idx):\n",
        "            i = batch_idx\n",
        "            epoch = self.current_epoch\n",
        "            image, target = batch['image'], batch['label']\n",
        "            target[target==254]=255\n",
        "            self.scheduler(self.optimizer, i, epoch, self.best_pred)\n",
        "            output = self.forward(image)\n",
        "            celoss = self.criterion(output, target.long())\n",
        "\n",
        "            # x, _ = self.encoder(image)\n",
        "            # coarse_output = self.decoder.coarse_cls(x)\n",
        "            # coarse_output = F.interpolate(coarse_output, size=image.size()[2:], mode='bilinear', align_corners=True)\n",
        "            # coarse_celoss = self.criterion(coarse_output, target.long())\n",
        "\n",
        "            self.log('train/ce', celoss.item())\n",
        "            # self.log('train/course_ce', coarse_celoss.item())\n",
        "            return celoss #+ coarse_celoss*0.1\n",
        "    \n",
        "    def get_loss_val(self, batch, batch_idx):\n",
        "            image, target = batch['image'], batch['label']\n",
        "            target[target==254]=255\n",
        "            i= batch_idx % len(batch['image'])\n",
        "            output = self.forward(image)\n",
        "            celoss = self.criterion(output, target.long())\n",
        "            mask = torch.max(output[i].unsqueeze(0),1)[1].detach()\n",
        "            self.val_img_logs += [wb_mask(image[i].cpu().numpy().transpose([1,2,0]), mask[0].cpu().numpy(), target[i].cpu().numpy())]\n",
        "\n",
        "            # x, _ = self.encoder(image)\n",
        "            # coarse_output = self.decoder.coarse_cls(x)\n",
        "            # coarse_output = F.interpolate(coarse_output, size=image.size()[2:], mode='bilinear', align_corners=True)\n",
        "            # mask = torch.max(coarse_output[i].unsqueeze(0),1)[1].detach()\n",
        "            # self.val_img_logs += [wb_mask(image[i].cpu().numpy().transpose([1,2,0]), mask[0].cpu().numpy(), target[i].cpu().numpy())]\n",
        "\n",
        "            pred = output.data.cpu().numpy()\n",
        "            pred = np.argmax(pred, axis=1)\n",
        "            target = target.cpu().numpy()\n",
        "            self.evaluator.add_batch(target, pred)\n",
        "            result = {\n",
        "              'ce_loss': celoss\n",
        "            }\n",
        "            return result\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        train_params = [{'params': self.get_1x_lr_params(), 'lr': self.hparams.lr},\n",
        "                        {'params': self.get_10x_lr_params(), 'lr': self.hparams.lr * 10}]\n",
        "        self.optimizer = torch.optim.SGD(train_params, momentum=self.hparams.momentum, \n",
        "                                                weight_decay=self.hparams.weight_decay, \n",
        "                                                nesterov=self.hparams.nesterov)\n",
        "        self.scheduler = LR_Scheduler(self.hparams.lr_scheduler, self.hparams.lr,\n",
        "                                            self.hparams.epochs, self.hparams.num_img_tr)\n",
        "        return self.optimizer\n",
        "\n",
        "    def get_1x_lr_params(self):\n",
        "        modules = [self.encoder.encoder]\n",
        "        for i in range(len(modules)):\n",
        "            for m in modules[i].named_modules():\n",
        "                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n",
        "                        or isinstance(m[1], nn.BatchNorm2d):\n",
        "                    for p in m[1].parameters():\n",
        "                        if p.requires_grad:\n",
        "                            yield p\n",
        "\n",
        "    def get_10x_lr_params(self):\n",
        "        modules = [self.decoder, self.encoder.aspp]\n",
        "        for i in range(len(modules)):\n",
        "            for m in modules[i].named_modules():\n",
        "                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n",
        "                        or isinstance(m[1], nn.BatchNorm2d):\n",
        "                    for p in m[1].parameters():\n",
        "                        if p.requires_grad:\n",
        "                            yield p\n"
      ],
      "metadata": {
        "id": "H_3wfwxWz0Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.batch_size = 10\n",
        "args.limit_dataset = False\n",
        "args.nclass = nclass\n",
        "args.lr = 0.01\n",
        "args.epochs = 1\n",
        "# train_loader, val_loader, test_loader, nclanss = make_data_loader(args, **kwargs)\n",
        "train_loader, val_loader = single_train_loader, single_val_loader\n",
        "\n",
        "args.num_img_tr=len(train_loader)\n",
        "model = ColorModel(args, encoder=DeepLabEncoder(args))\n",
        "# model = SegModelDebug(args)\n",
        "# model.configure_optimizers()\n",
        "# for param in model.encoder.model.backbone.parameters():\n",
        "#   param.requires_grad = False\n",
        "\n",
        "wandb_logger = WandbLogger(project='Color-Query', name='ffc_dlv3+') # proto_dlv3_rand-space  deeplabv3+_seeds  prototype_aspp_seeds  deeplabv3+_color-low-feats\n",
        "\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=1, logger=wandb_logger, log_every_n_steps=10, num_sanity_val_steps=0, progress_bar_refresh_rate=0, accumulate_grad_batches=1)\n",
        "results = trainer.fit(model, train_loader, val_loader)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "NfYLmnEQz8ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import gc\n",
        "# for obj in gc.get_objects():\n",
        "#     try:\n",
        "#         if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
        "#             print(type(obj), obj.size())\n",
        "#     except:\n",
        "#         pass"
      ],
      "metadata": {
        "id": "TUQSe-yj1Nzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dXZDsraULULp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}